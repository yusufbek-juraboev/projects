{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'poems.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 9964421 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never Enough of Living\n",
      "Never, my heart, is there enough of living,\n",
      "Since only in thee is loveliness so sweet pain;\n",
      "Only for thee the willows will be giving\n",
      "Their quiet fringes to the dreaming river;\n",
      "Only for thee so the light grasses ever\n",
      "Are h\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the tf.keras.layers.StringLookup layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prediction task\n",
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the outputâ€”the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
    "\n",
    "### Create training examples and targets\n",
    "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
    "\n",
    "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9964421,), dtype=int64, numpy=array([50, 73, 90, ...,  2,  3,  2], dtype=int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "E\n",
      "n\n",
      "o\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'N' b'e' b'v' b'e' b'r' b' ' b'E' b'n' b'o' b'u' b'g' b'h' b' ' b'o'\n",
      " b'f' b' ' b'L' b'i' b'v' b'i' b'n' b'g' b'\\r' b'\\n' b'N' b'e' b'v' b'e'\n",
      " b'r' b',' b' ' b'm' b'y' b' ' b'h' b'e' b'a' b'r' b't' b',' b' ' b'i'\n",
      " b's' b' ' b't' b'h' b'e' b'r' b'e' b' ' b'e' b'n' b'o' b'u' b'g' b'h'\n",
      " b' ' b'o' b'f' b' ' b'l' b'i' b'v' b'i' b'n' b'g' b',' b'\\r' b'\\n' b'S'\n",
      " b'i' b'n' b'c' b'e' b' ' b'o' b'n' b'l' b'y' b' ' b'i' b'n' b' ' b't'\n",
      " b'h' b'e' b'e' b' ' b'i' b's' b' ' b'l' b'o' b'v' b'e' b'l' b'i' b'n'\n",
      " b'e' b's' b's'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Never Enough of Living\\r\\nNever, my heart, is there enough of living,\\r\\nSince only in thee is loveliness'\n",
      "b' so sweet pain;\\r\\nOnly for thee the willows will be giving\\r\\nTheir quiet fringes to the dreaming river;'\n",
      "b'\\r\\nOnly for thee so the light grasses ever\\r\\nAre hollowed by the print of windy feet,\\r\\nAnd breathe hill'\n",
      "b' weather on the misty plain;\\r\\nAnd were no rapture of them in thy beat,\\r\\nFor every hour of sky\\r\\nStillb'\n",
      "b'orn in gladness would the waters wear\\r\\nColors of air translucently,\\r\\nAnd the stars sleep there.\\r\\nGent'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'Never Enough of Living\\r\\nNever, my heart, is there enough of living,\\r\\nSince only in thee is lovelines'\n",
      "Target: b'ever Enough of Living\\r\\nNever, my heart, is there enough of living,\\r\\nSince only in thee is loveliness'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 160) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  40960     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  164000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,143,264\n",
      "Trainable params: 4,143,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77,  58, 137,  18,  37, 126, 136, 113, 104, 129,  99,  36,  83,\n",
       "        45,  88,  14,  78,   3,  35,  40, 107,  51,  14,  91,  48,  82,\n",
       "       133,  92, 105, 114,  75,  88,  86,  12, 151, 128, 136, 123,  22,\n",
       "        98,  78,  49, 117,  34,   8,  29,  31, 121, 109,  68,  14,  19,\n",
       "        69,  51,   6,  20, 109, 127,  42, 101, 109, 149,  81, 151,  74,\n",
       "        87,  94, 112,  33, 123,  73, 132,  87, 120,  67, 116,  92,  24,\n",
       "         1,  16,  33,  41,   6, 100,   0, 114, 118,  57,  97,  83,  76,\n",
       "       150,  79,  75,  25, 141,  94, 102,  45, 121], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'he\\r\\ndetails of death\\xe2\\x80\\x99s arrival at a defenseless moment.\\r\\nCan you hear the snow as it settles its dri'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'iV\\xc3\\xb4.A\\xc3\\xa6\\xc3\\xb3\\xc2\\xbb\\xc2\\xab\\xc3\\xa9\\xc2\\xa0@oIt*j\\r?D\\xc2\\xb0O*wLn\\xc3\\xaex\\xc2\\xad\\xc3\\x80gtr(\\xe2\\x80\\x9d\\xc3\\xa8\\xc3\\xb3\\xc3\\xa22~jM\\xc3\\x86>$9;\\xc3\\xa0\\xc2\\xb4`*/aO\"0\\xc2\\xb4\\xc3\\xa7F\\xc2\\xa2\\xc2\\xb4\\xe2\\x80\\x99m\\xe2\\x80\\x9dfsz\\xc2\\xb9=\\xc3\\xa2e\\xc3\\xads\\xc3\\x89_\\xc3\\x85x4\\t,=E\"\\xc2\\xa1[UNK]\\xc3\\x80\\xc3\\x87U}oh\\xe2\\x80\\x9ckg5\\xc3\\xbbz\\xc2\\xa6I\\xc3\\xa0'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 160)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(5.075081, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159.98512"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1541/1541 [==============================] - 3656s 2s/step - loss: 1.1346\n",
      "Epoch 2/25\n",
      "1541/1541 [==============================] - 3881s 3s/step - loss: 1.1253\n",
      "Epoch 3/25\n",
      "1541/1541 [==============================] - 3896s 3s/step - loss: 1.1345\n",
      "Epoch 4/25\n",
      "1541/1541 [==============================] - 3893s 3s/step - loss: 1.1222\n",
      "Epoch 5/25\n",
      "1541/1541 [==============================] - 3905s 3s/step - loss: 1.1226\n",
      "Epoch 6/25\n",
      "1541/1541 [==============================] - 3886s 3s/step - loss: 1.1338\n",
      "Epoch 7/25\n",
      "1541/1541 [==============================] - 3892s 3s/step - loss: 1.2266\n",
      "Epoch 8/25\n",
      "1541/1541 [==============================] - 3891s 3s/step - loss: 1.3447\n",
      "Epoch 9/25\n",
      "1541/1541 [==============================] - 3900s 3s/step - loss: 1.3464\n",
      "Epoch 10/25\n",
      "1541/1541 [==============================] - 3979s 3s/step - loss: 1.2050\n",
      "Epoch 11/25\n",
      "1541/1541 [==============================] - 4218s 3s/step - loss: 1.2365\n",
      "Epoch 12/25\n",
      "1541/1541 [==============================] - 4314s 3s/step - loss: 1.2985\n",
      "Epoch 13/25\n",
      "1541/1541 [==============================] - 4233s 3s/step - loss: 1.3279\n",
      "Epoch 14/25\n",
      "1541/1541 [==============================] - 4077s 3s/step - loss: 1.4635\n",
      "Epoch 15/25\n",
      "1541/1541 [==============================] - 3996s 3s/step - loss: 2.0012\n",
      "Epoch 16/25\n",
      "1541/1541 [==============================] - 3924s 3s/step - loss: 1.9799\n",
      "Epoch 17/25\n",
      "1541/1541 [==============================] - 4784s 3s/step - loss: 1.9939\n",
      "Epoch 18/25\n",
      "1541/1541 [==============================] - 4134s 3s/step - loss: 1.9511\n",
      "Epoch 19/25\n",
      "1541/1541 [==============================] - 4169s 3s/step - loss: 1.9287\n",
      "Epoch 20/25\n",
      "1541/1541 [==============================] - 4357s 3s/step - loss: 1.8829\n",
      "Epoch 21/25\n",
      "1541/1541 [==============================] - 4784s 3s/step - loss: 1.8180\n",
      "Epoch 22/25\n",
      "1541/1541 [==============================] - 4820s 3s/step - loss: 1.7397\n",
      "Epoch 23/25\n",
      "1541/1541 [==============================] - 4370s 3s/step - loss: 1.6499\n",
      "Epoch 24/25\n",
      "1541/1541 [==============================] - 4371s 3s/step - loss: 1.5562\n",
      "Epoch 25/25\n",
      "1541/1541 [==============================] - 4146s 3s/step - loss: 1.4650\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Humpty dumpty'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy valentine-slaveâ€™s stones on the confused,\n",
      "me, my death your reâ€™d need in unjustion\n",
      "Again,   \n",
      "But youâ€™re here his clay for suckets,\n",
      "Recalling the rain burning sleep-start.\n",
      "But weâ€™ve ladies. \n",
      "\n",
      "My Fretowin her de through the Garden.\n",
      "So stry clean in bore it spell.\n",
      "I move Over a short cance ignoinias their born by tears far afterman passions and in the road\n",
      "The last before the stretchislection.\n",
      "Now their fair, agian sadness,\n",
      "With my people play recall\n",
      "To follows Blind...\n",
      "A all weak in said it all of other.\n",
      "In a morning,\n",
      "than that my superstition.\n",
      "Say that moment you not dalky,\n",
      "Dead us from tended...\n",
      "Like make head pains, like eet gife-his heart â€“ but in enter,\n",
      "a net youâ€™ll feel:\n",
      "we have like a way out to other.\n",
      "\n",
      "The Returning of Love\n",
      "Confining friend.\n",
      "Millier in your burning of far to remember or tears,\n",
      "A reflections hot windows, pent, ratter,\n",
      "Then a boxt to right my life soul, not enly noise singer\n",
      "the snow, my decide\n",
      "lost immebird spealt pines of people bard.\n",
      "And Iâ€™m \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 9.483880519866943\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Happy valentine'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
